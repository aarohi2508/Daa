<!DOCTYPE html>
<html>
<head>
<title>DAA</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>
<body>



<div class="container-fluid">
   <div class="row">
       <div class="col-md-2 col-sm-12 col-xs-12">
           <nav id="navbar">
               <header>DESIGN ALGORITHM ANALYSIS </header>
               <ul class="nav nav-pills nav-stacked">
                   <a class="nav-link" href="#Sorting Algorithm" rel="internal">
                       <li>Sorting Algorithm</li>
                   </a>
                   <a class="nav-link" href="#Searching" rel="internal">
                       <li>Searching</li>
                   </a>
                   <a class="nav-link" href="#N Queen's Problem" rel="internal">
                       <li>N Queen's Problem</li>
                   </a>
                   <a class="nav-link" href="#Dynamic Programming" rel="internal">
                       <li>Dynamic Programming</li>
                   </a>
                   <a class="nav-link" href="#Knapsack problem" rel="internal">
                       <li>Knapsack Problem</li>
                   </a>
                   <a class="nav-link" href="#greedy programming" rel="internal">
                       <li>Greedy Programming</li>
                   </a>
                   <a class="nav-link" href="#travelling salesman problem" rel="internal">
                       <li>Travelling Salesman Problem</li>
                   </a>
                  
                   <a class="nav-link" href="#hamiltonian circuits" rel="internal">
                       <li>Hamiltonian circuits</li>
                   </a>
                   
                   <a class="nav-link" href="#convex hull" rel="internal">
                       <li>Convex Hull</li>
                   </a>
                   <a class="nav-link" href="#optimal binary serach tree" rel="internal">
                       <li>Optimal Binary Search Tree</li>
                   </a>
                   <a class="nav-link" href="#Floyd warshall" rel="internal">
                       <li>Floyd Warshall</li>
                   </a>
               </ul>

           </nav>
       </div>

       <div class="col-md-10 col-sm-12 col-xs-12">
           <main id="main-doc">
               <section class="main-section" id="sorting algorithm">
                   <header>Sorting Algorithm</header>
                   <article>
                       <p>Sorting refers to arranging data in a particular format. Sorting algorithm specifies the way to arrange data in a particular order. Most common orders are in numerical or lexicographical order.

                        The importance of sorting lies in the fact that data searching can be optimized to a very high level, if data is stored in a sorted manner. Sorting is also used to represent data in more readable formats.</p>
<li>Bubble sort </li>
<li>Insertion sort</li>
<li>Quick sort</li>
<li>Merge sort</li>
<li>Selection sort</li>
                       
                       </article>
               </section>
               <section class="main-section" id="seraching">
                   <header>Searching</header>
                   <article>
                       <p>Searching is the process of finding a given value position in a list of values.
                        It decides whether a search key is present in the data or not.
                        It is the algorithmic process of finding a particular item in a collection of items.
                        It can be done on internal data structure or on external data structure.</p>

                       <li>Linear Search</li>
                       <code>function searchValue(value, target)
                        {
                              for (var i = 0; i < value.length; i++)
                              {
                                     if (value[i] == target)
                                     {
                                             return i;
                                     }
                              }
                              return -1;
                        }</code>
                       <li>Binary Search</li>
                       <code>function binary_search(A, n, T) is
                        L := 0
                        R := n − 1
                        while L ≤ R do
                            m := floor((L + R) / 2)
                            if A[m] < T then
                                L := m + 1
                            else if A[m] > T then
                                R := m - 1
                            else:
                                return m
                        return unsuccessful</code>
                       </article>
               </section>
               <section class="main-section" id="N Queens problem">
                   <header>N Queen's Problem</header>
                   <article>
                       <p>This problem is to find an arrangement of N queens on a chess board, such that no queen can attack any other queens on the board.

                        The chess queens can attack in any direction as horizontal, vertical, horizontal and diagonal way.
                        
                        A binary matrix is used to display the positions of N Queens, where no queens can attack other queens.</p>

                       <code>Begin
                        if all columns are filled, then
                           return true
                        for each row of the board, do
                           if isValid(board, i, col), then
                              set queen at place (i, col) in the board
                              if solveNQueen(board, col+1) = true, then
                                 return true
                              otherwise remove queen from place (i, col) from board.
                        done
                        return false
                     End</code>
                       
                   </article>
               </section>
               <section class="main-section" id="Dynamic Programming">
                   <header>Dynamic Programming</header>
                   <article>
                        <p>Dynamic Programming is mainly an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls for same inputs, we can optimize it using Dynamic Programming. The idea is to simply store the results of subproblems, so that we do not have to re-compute them when needed later. This simple optimization reduces time complexities from exponential to polynomial. For example, if we write simple recursive solution for Fibonacci Numbers, we get exponential time complexity and if we optimize it by storing solutions of subproblems, time complexity reduces to linear.</p>
                   <p>The following computer problems can be solved using dynamic programming approach −

                     <li>Fibonacci number series</li>
                     <li>Knapsack problem</li>
                     <li>Tower of Hanoi</li>
                    </p>
                     </article>
               </section>
               <section class="main-section" id="Knapsack problem">
                   <header>Knapsack Problem</header>
                   <p>
                     Given a set of items, each with a weight and a value, determine a subset of items to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.

                     The knapsack problem is in combinatorial optimization problem. It appears as a subproblem in many, more complex mathematical models of real-world problems. One general approach to difficult problems is to identify the most restrictive constraint, ignore the others, solve a knapsack problem, and somehow adjust the solution to satisfy the ignored constraints.
                   </p>
                   <p>
                     Knapsack problems are categorized as

                    <li> Fractional Knapsack</li>
                     <li>Knapsack</li>
                   </p>
                   <h>Fractional Knapsack Problem</h>
                       <p>In this case, items can be broken into smaller pieces, hence the thief can select fractions of items.

                        According to the problem statement,
                        
                        There are n items in the store
                        
                        Weight of ith item wi>0
                        Profit for ith item pi>0 and
                        
                        Capacity of the Knapsack is W
                        
                        In this version of Knapsack problem, items can be broken into smaller pieces. So, the thief may take only a fraction xi of ith item.
                        
                        0⩽xi⩽1
                        The ith item contributes the weight xi.wi to the total weight in the knapsack and profit xi.pi to the total profit.
                        
                        Hence, the objective of this algorithm is to
                        
                        maximize∑n=1n(xi.pi)
                        subject to constraint,
                        
                        ∑n=1n(xi.wi)⩽W
                        It is clear that an optimal solution must fill the knapsack exactly, otherwise we could add a fraction of one of the remaining items and increase the overall profit.
                        
                        Thus, an optimal solution can be obtained by
                        
                        ∑n=1n(xi.wi)=W
                        In this context, first we need to sort those items according to the value of piwi, so that pi+1wi+1 ≤ piwi . Here, x is an array to store the fraction of items.</p>
<code>Algorithm: Greedy-Fractional-Knapsack (w[1..n], p[1..n], W) 
   for i = 1 to n 
      do x[i] = 0 
   weight = 0 
   for i = 1 to n 
      if weight + w[i] ≤ W then  
         x[i] = 1 
         weight = weight + w[i] 
      else 
         x[i] = (W - weight) / w[i] 
         weight = W 
         break 
   return x</code>
   <p>In 0-1 Knapsack, items cannot be broken which means the thief should take the item as a whole or should leave it. This is reason behind calling it as 0-1 Knapsack.

      Hence, in case of 0-1 Knapsack, the value of xi can be either 0 or 1, where other constraints remain the same.
      
      0-1 Knapsack cannot be solved by Greedy approach. Greedy approach does not ensure an optimal solution. In many instances, Greedy approach may give an optimal solution.
      
      
      </p>
      <code>Dynamic-0-1-knapsack (v, w, n, W) 
         for w = 0 to W do 
            c[0, w] = 0 
         for i = 1 to n do 
            c[i, 0] = 0 
            for w = 1 to W do 
               if wi ≤ w then 
                  if vi + c[i-1, w-wi] then 
                     c[i, w] = vi + c[i-1, w-wi] 
                  else c[i, w] = c[i-1, w] 
               else 
                  c[i, w] = c[i-1, w] </code>
               </section>
               <section class="main-section" id="Greedy programming">
                   <header>Greedy Programming</header>
                   <article>
                       <p>A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage[1] with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.

                        For example, a greedy strategy for the travelling salesman problem (which is of a high computational complexity) is the following heuristic: "At each step of the journey, visit the nearest unvisited city." This heuristic does not intend to find a best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids, and give constant-factor approximations to optimization problems with submodular structure.</p>
                   </article>
               </section>
               <section class="main-section" id="Travelling salesman problem">
                   <header>Travelling Salesman Problem</header>
                   <article>
                       <p> WThe travelling salesman problem (also called the travelling salesperson problem[1] or TSP) asks the following question: "Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?" It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.

                        The travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.
                        
                        In the theory of computational complexity, the decision version of the TSP (where given a length L, the task is to decide whether the graph has a tour of at most L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.
                        
                        The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, many heuristics and exact algorithms are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.[2]
                        
                        The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimize the time spent moving the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.</p>

                       
                   </article>
               </section>
               
               <section class="main-section" id="Hamiltonian circuits">
                   <header>Hamiltonian circuits</header>
                   <article>
                       <p>Hamiltonian Path in an undirected graph is a path that visits each vertex exactly once. A Hamiltonian cycle (or Hamiltonian circuit) is a Hamiltonian Path such that there is an edge (in the graph) from the last vertex to the first vertex of the Hamiltonian Path. Determine whether a given graph contains Hamiltonian Cycle or not. If it contains, then prints the path. Following are the input and output of the required function.

                        Input:
                        A 2D array graph[V][V] where V is the number of vertices in graph and graph[V][V] is adjacency matrix representation of the graph. A value graph[i][j] is 1 if there is a direct edge from i to j, otherwise graph[i][j] is 0.
                        
                        Output:
                        An array path[V] that should contain the Hamiltonian Path. path[i] should represent the ith vertex in the Hamiltonian Path. The code should also return false if there is no Hamiltonian Cycle in the graph.</p>
            
                   </article>
               </section>
               
               <section class="main-section" id="convex hull">
                   <header>Convex Hull</header>
                   <article>
                       <p>The convex hull or convex envelope or convex closure of a shape is the smallest convex set that contains it. The convex hull may be defined either as the intersection of all convex sets containing a given subset of a Euclidean space, or equivalently as the set of all convex combinations of points in the subset. For a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around the subset.

                        Convex hulls of open sets are open, and convex hulls of compact sets are compact. Every convex set is the convex hull of its extreme points. The convex hull operator is an example of a closure operator, and every antimatroid can be represented by applying this closure operator to finite sets of points. The algorithmic problems of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces, and its dual problem of intersecting half-spaces, are fundamental problems of computational geometry. They can be solved in time {\displaystyle O(n\log n)}O(n\log n) for two or three dimensional point sets, and in time matching the worst-case output complexity given by the upper bound theorem in higher dimensions.
                        
                        As well as for finite point sets, convex hulls have also been studied for simple polygons, Brownian motion, space curves, and epigraphs of functions. Convex hulls have wide applications in mathematics, statistics, combinatorial optimization, economics, geometric modeling, and ethology. Related structures include the orthogonal convex hull, convex layers, Delaunay triangulation and Voronoi diagram, and convex skull.</p>
                   </article>
                  </section>
                <section class="main-section" id="Optimal-Binary-Search-Tree">
                    <header>Optimal Binary Search Tree</header>
                    <article>
                      <p>An optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree,[1] is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). Optimal BSTs are generally divided into two types: static and dynamic.

                        In the static optimality problem, the tree cannot be modified after it has been constructed. In this case, there exists some particular layout of the nodes of the tree which provides the smallest expected search time for the given access probabilities. Various algorithms exist to construct or approximate the statically optimal tree given the information on the access probabilities of the elements.
                        
                        In the dynamic optimality problem, the tree can be modified at any time, typically by permitting tree rotations. The tree is considered to have a cursor starting at the root which it can move or use to perform modifications. In this case, there exists some minimal-cost sequence of these operations which causes the cursor to visit every node in the target access sequence in order. The splay tree is conjectured to have a constant competitive ratio compared to the dynamically optimal tree in all cases, though this has not yet been proven</p>
                  <code>
                     Optimal-Binary-Search-Tree(p, q, n) 
e[1…n + 1, 0…n],  
w[1…n + 1, 0…n], 
root[1…n + 1, 0…n]  
for i = 1 to n + 1 do 
   e[i, i - 1] := qi - 1 
   w[i, i - 1] := qi - 1  
for l = 1 to n do 
   for i = 1 to n – l + 1 do 
      j = i + l – 1 e[i, j] := ∞ 
      w[i, i] := w[i, i -1] + pj + qj 
      for r = i to j do 
         t := e[i, r - 1] + e[r + 1, j] + w[i, j] 
         if t < e[i, j] 
            e[i, j] := t 
            root[i, j] := r 
return e and root
                  </code>
                     </article>
                </section>
                <section class="main-section" id="Floyd warshall">
                    <header>Floyd Warshall</header>
                    <article>
                        <p>Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). It does so by comparing all possible paths through the graph between each pair of vertices and that too with O(V3) comparisons in a graph.

                           Below is the psedocode for Floyd Warshall as given in wikipedia. The implementation takes in a graph, represented by adjacency matrix and fills dist[] with shortest-path (least cost) information </p>
                   <code>let dist be a V x V matrix of minimum distances initialized to infinity
                     for each vertex v
                         dist[v][v] = 0
                     for each edge (u, v)
                         dist[u][v] = weight(u,v)
                      
                     for k from 0 to |V| – 1
                         for i from 0 to |V| – 1
                             for j from 0 to |V| – 1
                                 if (dist[i][k] + dist[k][j]) is less than dist[i][j] then
                                     dist[i][j] = dist[i][k] + dist[k][j]</code>
                        </article>
                </section>
            </main>
        </div>
    </div>
</div>
</body>
</html>

